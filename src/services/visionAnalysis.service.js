import axios from 'axios';
import fs from 'fs/promises';
import { z } from 'zod';
import { AppError } from '../middleware/errorHandler.js';

const VISION_API_KEY = process.env.VISION_API_KEY;
const VISION_API_URL = process.env.VISION_API_URL || 'https://api.openai.com/v1/chat/completions';
const VISION_MODEL = process.env.VISION_MODEL || 'gpt-4-vision-preview';

// å®šç¾©èªç¾©åˆ†æçš„ enum å’Œ schema
export const ShotTypeEnum = z.enum(['close_up', 'medium', 'wide', 'screen', 'broll']);
export const SubjectEnum = z.enum(['human_face', 'human_body', 'screen_ui', 'object', 'text_only']);
export const SubtitleEnum = z.enum(['none', 'short_hook', 'sentence', 'paragraph']);
export const EmotionEnum = z.enum(['curiosity', 'excitement', 'explanation', 'tension', 'calm']);
export const MotionEnum = z.enum(['static', 'slight_motion', 'strong_motion']);

const SemanticAnalysisSchema = z.object({
    shot_type: ShotTypeEnum,
    subject: SubjectEnum,
    subtitle: SubtitleEnum,
    emotion: EmotionEnum,
    motion: MotionEnum
});

/**
 * æ§‹å»ºè¦–è¦ºèªç¾©åˆ†æçš„ promptï¼ˆå°é–‰å¼å•é¡Œï¼‰
 */
const buildSemanticPrompt = () => {
    return `è«‹åˆ†æé€™å¼µå½±ç‰‡æˆªåœ–ï¼Œä¸¦ç”¨ä»¥ä¸‹ enum å›ç­”ï¼Œä¸è¦è§£é‡‹ï¼Œåªå›ç­” JSON æ ¼å¼ï¼š

{
  "shot_type": "[close_up, medium, wide, screen, broll] é¸ä¸€å€‹",
  "subject": "[human_face, human_body, screen_ui, object, text_only] é¸ä¸€å€‹",
  "subtitle": "[none, short_hook, sentence, paragraph] é¸ä¸€å€‹",
  "emotion": "[curiosity, excitement, explanation, tension, calm] é¸ä¸€å€‹",
  "motion": "[static, slight_motion, strong_motion] é¸ä¸€å€‹"
}

å®šç¾©ï¼š
- shot_type: é¡é ­é¡å‹ï¼ˆç‰¹å¯«/ä¸­æ™¯/é æ™¯/è¢å¹•/B-rollï¼‰
- subject: ä¸»é«”ï¼ˆäººè‡‰/äººé«”/è¢å¹•UI/ç‰©é«”/ç´”æ–‡å­—ï¼‰
- subtitle: å­—å¹•å¯†åº¦ï¼ˆç„¡/çŸ­é‰¤å­/å¥å­/æ®µè½ï¼‰
- emotion: è¦–è¦ºæƒ…ç·’ï¼ˆå¥½å¥‡/èˆˆå¥®/è§£é‡‹/ç·Šå¼µ/å¹³éœï¼‰
- motion: ç•«é¢å‹•æ…‹æ„Ÿï¼ˆéœæ…‹/è¼•å¾®å‹•æ…‹/å¼·çƒˆå‹•æ…‹ï¼‰

åªå›ç­” JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚`;
};

/**
 * åˆ†æå–®å¼µé—œéµå¹€çš„èªç¾©
 * @param {string} imagePath - åœ–ç‰‡è·¯å¾‘
 * @param {number} retries - é‡è©¦æ¬¡æ•¸
 * @returns {Promise<Object>} - èªç¾©åˆ†æçµæœ
 */
export const analyzeFrameSemantics = async (imagePath, retries = 0) => {
    try {
        // è®€å–åœ–ç‰‡ä¸¦è½‰æ›ç‚º base64
        const imageBuffer = await fs.readFile(imagePath);
        const base64Image = imageBuffer.toString('base64');

        // èª¿ç”¨ Vision API
        const response = await axios.post(
            VISION_API_URL,
            {
                model: VISION_MODEL,
                messages: [
                    {
                        role: 'user',
                        content: [
                            { type: 'text', text: buildSemanticPrompt() },
                            {
                                type: 'image_url',
                                image_url: {
                                    url: `data:image/jpeg;base64,${base64Image}`
                                }
                            }
                        ]
                    }
                ],
                max_tokens: 300,
                temperature: 0.1 // ä½æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„çµæœ
            },
            {
                headers: {
                    'Authorization': `Bearer ${VISION_API_KEY}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            }
        );

        // æå–å›æ‡‰
        const content = response.data.choices[0].message.content;

        // è§£æ JSONï¼ˆå¯èƒ½éœ€è¦æ¸…ç†ï¼‰
        const jsonMatch = content.match(/\{[\s\S]*\}/);
        if (!jsonMatch) {
            throw new Error('No JSON found in response');
        }

        const parsed = JSON.parse(jsonMatch[0]);

        // Schema é©—è­‰ - å¦‚æœä¸åˆæ³•å°±é‡å•
        const validated = SemanticAnalysisSchema.parse(parsed);

        return validated;

    } catch (error) {
        // Retry é‚è¼¯
        if (retries < 2 && (error.name === 'ZodError' || error.message.includes('JSON'))) {
            console.log(`âš ï¸ Invalid response, retrying... (${retries + 1}/2)`);
            await new Promise(resolve => setTimeout(resolve, 1000));
            return analyzeFrameSemantics(imagePath, retries + 1);
        }

        console.error('Vision analysis error:', error.message);

        // è¿”å›é»˜èªå€¼è€Œä¸æ˜¯å¤±æ•—
        return {
            shot_type: 'medium',
            subject: 'object',
            subtitle: 'none',
            emotion: 'calm',
            motion: 'slight_motion',
            error: true
        };
    }
};

/**
 * æ‰¹é‡åˆ†æå¤šå€‹é—œéµå¹€
 * @param {Array} keyframes - é—œéµå¹€æ•¸çµ„
 * @returns {Promise<Array>} - èªç¾©åˆ†æçµæœæ•¸çµ„
 */
export const analyzeKeyframesSemantics = async (keyframes) => {
    const results = [];

    for (const keyframe of keyframes) {
        console.log(`ğŸ” Analyzing shot ${keyframe.shotId}...`);

        const semantics = await analyzeFrameSemantics(keyframe.path);

        results.push({
            shotId: keyframe.shotId,
            timestamp: keyframe.timestamp,
            ...semantics
        });

        // é¿å… API rate limit
        await new Promise(resolve => setTimeout(resolve, 500));
    }

    return results;
};
